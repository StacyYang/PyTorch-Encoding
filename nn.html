

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>encoding.nn &mdash; Encoding master documentation</title>
  

  
  
  
  

  
  <script type="text/javascript" src="_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
        <script src="_static/language_data.js"></script>
        <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    
    <script type="text/javascript" src="_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lato" type="text/css" />
  <link rel="stylesheet" href="_static/css/encoding.css" type="text/css" />
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="encoding.parallel" href="parallel.html" />
    <link rel="prev" title="Deep TEN: Deep Texture Encoding Network Example" href="experiments/texture.html" /> 


  <script type="text/javascript" src="../_static/js/hidebib.js"></script>    

</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="index.html">
          

          
            
            <img src="_static/icon.png" class="logo" alt="Logo"/>
          
          </a>

          
            
            
              <div class="version">
                master (1.1.1b06042020)
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Notes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="notes/compile.html">Install and Citations</a><ul>
<li class="toctree-l2"><a class="reference internal" href="notes/compile.html#installation">Installation</a></li>
<li class="toctree-l2"><a class="reference internal" href="notes/compile.html#citations">Citations</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="notes/syncbn.html">Implementing Synchronized Multi-GPU Batch Normalization</a><ul>
<li class="toctree-l2"><a class="reference internal" href="notes/syncbn.html#how-bn-works">How BN works?</a></li>
<li class="toctree-l2"><a class="reference internal" href="notes/syncbn.html#why-synchronize-bn">Why Synchronize BN?</a></li>
<li class="toctree-l2"><a class="reference internal" href="notes/syncbn.html#how-to-synchronize">How to Synchronize?</a></li>
<li class="toctree-l2"><a class="reference internal" href="notes/syncbn.html#citation">Citation</a></li>
</ul>
</li>
</ul>
<p class="caption"><span class="caption-text">Experiment Systems</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="experiments/cifar.html">EncNet on CIFAR-10</a><ul>
<li class="toctree-l2"><a class="reference internal" href="experiments/cifar.html#test-pre-trained-model">Test Pre-trained Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="experiments/cifar.html#train-your-own-model">Train Your Own Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="experiments/cifar.html#citation">Citation</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="experiments/segmentation.html">Context Encoding for Semantic Segmentation (EncNet)</a><ul>
<li class="toctree-l2"><a class="reference internal" href="experiments/segmentation.html#install-package">Install Package</a></li>
<li class="toctree-l2"><a class="reference internal" href="experiments/segmentation.html#test-pre-trained-model">Test Pre-trained Model</a><ul>
<li class="toctree-l3"><a class="reference internal" href="experiments/segmentation.html#quick-demo">Quick Demo</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="experiments/segmentation.html#train-your-own-model">Train Your Own Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="experiments/segmentation.html#citation">Citation</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="experiments/style.html">MSG-Net Style Transfer Example</a><ul>
<li class="toctree-l2"><a class="reference internal" href="experiments/style.html#tabe-of-content">Tabe of content</a></li>
<li class="toctree-l2"><a class="reference internal" href="experiments/style.html#msg-net">MSG-Net</a></li>
<li class="toctree-l2"><a class="reference internal" href="experiments/style.html#stylize-images-using-pre-trained-model">Stylize Images Using Pre-trained Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="experiments/style.html#train-your-own-msg-net-model">Train Your Own MSG-Net Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="experiments/style.html#neural-style">Neural Style</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="experiments/texture.html">Deep TEN: Deep Texture Encoding Network Example</a><ul>
<li class="toctree-l2"><a class="reference internal" href="experiments/texture.html#test-pre-trained-model">Test Pre-trained Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="experiments/texture.html#train-your-own-model">Train Your Own Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="experiments/texture.html#citation">Citation</a></li>
</ul>
</li>
</ul>
<p class="caption"><span class="caption-text">Package Reference</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">encoding.nn</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#encoding"><span class="hidden-section">Encoding</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="#syncbatchnorm"><span class="hidden-section">SyncBatchNorm</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="#batchnorm1d"><span class="hidden-section">BatchNorm1d</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="#batchnorm2d"><span class="hidden-section">BatchNorm2d</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="#batchnorm3d"><span class="hidden-section">BatchNorm3d</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="#inspiration"><span class="hidden-section">Inspiration</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="#upsampleconv2d"><span class="hidden-section">UpsampleConv2d</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="#grammatrix"><span class="hidden-section">GramMatrix</span></a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="parallel.html">encoding.parallel</a><ul>
<li class="toctree-l2"><a class="reference internal" href="parallel.html#dataparallelmodel"><span class="hidden-section">DataParallelModel</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="parallel.html#dataparallelcriterion"><span class="hidden-section">DataParallelCriterion</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="parallel.html#allreduce"><span class="hidden-section">allreduce</span></a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="models.html">encoding.models</a><ul>
<li class="toctree-l2"><a class="reference internal" href="models.html#resnet">ResNet</a><ul>
<li class="toctree-l3"><a class="reference internal" href="models.html#id1"><span class="hidden-section">ResNet</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="models.html#resnet18"><span class="hidden-section">resnet18</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="models.html#resnet34"><span class="hidden-section">resnet34</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="models.html#resnet50"><span class="hidden-section">resnet50</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="models.html#resnet101"><span class="hidden-section">resnet101</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="models.html#resnet152"><span class="hidden-section">resnet152</span></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="utils.html">encoding.utils</a><ul>
<li class="toctree-l2"><a class="reference internal" href="utils.html#lr-scheduler"><span class="hidden-section">LR_Scheduler</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="utils.html#save-checkpoint"><span class="hidden-section">save_checkpoint</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="utils.html#batch-pix-accuracy"><span class="hidden-section">batch_pix_accuracy</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="utils.html#batch-intersection-union"><span class="hidden-section">batch_intersection_union</span></a></li>
</ul>
</li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">Encoding</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html">Docs</a> &raquo;</li>
        
      <li>encoding.nn</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="_sources/nn.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="encoding-nn">
<h1>encoding.nn<a class="headerlink" href="#encoding-nn" title="Permalink to this headline">¶</a></h1>
<p>Customized NN modules in Encoding Package. For Synchronized Cross-GPU Batch Normalization, please visit <a class="reference internal" href="#encoding.nn.BatchNorm2d" title="encoding.nn.BatchNorm2d"><code class="xref py py-class docutils literal notranslate"><span class="pre">encoding.nn.BatchNorm2d</span></code></a>.</p>
<div class="section" id="encoding">
<h2><span class="hidden-section">Encoding</span><a class="headerlink" href="#encoding" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt id="encoding.nn.Encoding">
<em class="property">class </em><code class="sig-prename descclassname">encoding.nn.</code><code class="sig-name descname">Encoding</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">D</span></em>, <em class="sig-param"><span class="n">K</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/encoding/nn/encoding.html#Encoding"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#encoding.nn.Encoding" title="Permalink to this definition">¶</a></dt>
<dd><p>Encoding Layer: a learnable residual encoder.</p>
<a class="reference internal image-reference" href="_images/cvpr171.svg"><img alt="_images/cvpr171.svg" class="align-center" src="_images/cvpr171.svg" width="50%" /></a>
<p>Encoding Layer accpets 3D or 4D inputs.
It considers an input featuremaps with the shape of <span class="math notranslate nohighlight">\(C\times H\times W\)</span>
as a set of C-dimentional input features <span class="math notranslate nohighlight">\(X=\{x_1, ...x_N\}\)</span>, where N is total number
of features given by <span class="math notranslate nohighlight">\(H\times W\)</span>, which learns an inherent codebook
<span class="math notranslate nohighlight">\(D=\{d_1,...d_K\}\)</span> and a set of smoothing factor of visual centers
<span class="math notranslate nohighlight">\(S=\{s_1,...s_K\}\)</span>. Encoding Layer outputs the residuals with soft-assignment weights
<span class="math notranslate nohighlight">\(e_k=\sum_{i=1}^Ne_{ik}\)</span>, where</p>
<div class="math notranslate nohighlight">
\[e_{ik} = \frac{exp(-s_k\|r_{ik}\|^2)}{\sum_{j=1}^K exp(-s_j\|r_{ij}\|^2)} r_{ik}\]</div>
<p>and the residuals are given by <span class="math notranslate nohighlight">\(r_{ik} = x_i - d_k\)</span>. The output encoders are
<span class="math notranslate nohighlight">\(E=\{e_1,...e_K\}\)</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>D</strong> – dimention of the features or feature channels</p></li>
<li><p><strong>K</strong> – number of codeswords</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Shape:</dt><dd><ul class="simple">
<li><p>Input: <span class="math notranslate nohighlight">\(X\in\mathcal{R}^{B\times N\times D}\)</span> or
<span class="math notranslate nohighlight">\(\mathcal{R}^{B\times D\times H\times W}\)</span> (where <span class="math notranslate nohighlight">\(B\)</span> is batch,
<span class="math notranslate nohighlight">\(N\)</span> is total number of features or <span class="math notranslate nohighlight">\(H\times W\)</span>.)</p></li>
<li><p>Output: <span class="math notranslate nohighlight">\(E\in\mathcal{R}^{B\times K\times D}\)</span></p></li>
</ul>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Variables</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>~Encoding.codewords</strong> (<em>Tensor</em>) – the learnable codewords of shape (<span class="math notranslate nohighlight">\(K\times D\)</span>)</p></li>
<li><p><strong>~Encoding.scale</strong> (<em>Tensor</em>) – the learnable scale factor of visual centers</p></li>
</ul>
</dd>
</dl>
<dl>
<dt>Reference:</dt><dd><p>Hang Zhang, Kristin Dana, Jianping Shi, Zhongyue Zhang, Xiaogang Wang, Ambrish Tyagi,
Amit Agrawal. “Context Encoding for Semantic Segmentation.
<em>The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) 2018</em></p>
<p>Hang Zhang, Jia Xue, and Kristin Dana. “Deep TEN: Texture Encoding Network.”
<em>The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) 2017</em></p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">encoding</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">torch.autograd</span> <span class="kn">import</span> <span class="n">Variable</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">B</span><span class="p">,</span><span class="n">C</span><span class="p">,</span><span class="n">H</span><span class="p">,</span><span class="n">W</span><span class="p">,</span><span class="n">K</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">6</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">DoubleTensor</span><span class="p">(</span><span class="n">B</span><span class="p">,</span><span class="n">C</span><span class="p">,</span><span class="n">H</span><span class="p">,</span><span class="n">W</span><span class="p">)</span><span class="o">.</span><span class="n">uniform_</span><span class="p">(</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span><span class="mf">0.5</span><span class="p">),</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">layer</span> <span class="o">=</span> <span class="n">encoding</span><span class="o">.</span><span class="n">Encoding</span><span class="p">(</span><span class="n">C</span><span class="p">,</span><span class="n">K</span><span class="p">)</span><span class="o">.</span><span class="n">double</span><span class="p">()</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">E</span> <span class="o">=</span> <span class="n">layer</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</pre></div>
</div>
<dl class="py method">
<dt id="encoding.nn.Encoding.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">X</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/encoding/nn/encoding.html#Encoding.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#encoding.nn.Encoding.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="syncbatchnorm">
<h2><span class="hidden-section">SyncBatchNorm</span><a class="headerlink" href="#syncbatchnorm" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt id="encoding.nn.SyncBatchNorm">
<em class="property">class </em><code class="sig-prename descclassname">encoding.nn.</code><code class="sig-name descname">SyncBatchNorm</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">num_features</span></em>, <em class="sig-param"><span class="n">eps</span><span class="o">=</span><span class="default_value">1e-05</span></em>, <em class="sig-param"><span class="n">momentum</span><span class="o">=</span><span class="default_value">0.1</span></em>, <em class="sig-param"><span class="n">sync</span><span class="o">=</span><span class="default_value">True</span></em>, <em class="sig-param"><span class="n">activation</span><span class="o">=</span><span class="default_value">'none'</span></em>, <em class="sig-param"><span class="n">slope</span><span class="o">=</span><span class="default_value">0.01</span></em>, <em class="sig-param"><span class="n">inplace</span><span class="o">=</span><span class="default_value">True</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/encoding/nn/syncbn.html#SyncBatchNorm"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#encoding.nn.SyncBatchNorm" title="Permalink to this definition">¶</a></dt>
<dd><p>Cross-GPU Synchronized Batch normalization (SyncBN)</p>
<p>Standard BN <a class="footnote-reference brackets" href="#id3" id="id1">1</a> implementation only normalize the data within each device (GPU).
SyncBN normalizes the input within the whole mini-batch.
We follow the sync-onece implmentation described in the paper <a class="footnote-reference brackets" href="#id4" id="id2">2</a> .
Please see the design idea in the <a class="reference external" href="./notes/syncbn.html">notes</a>.</p>
<div class="math notranslate nohighlight">
\[y = \frac{x - mean[x]}{ \sqrt{Var[x] + \epsilon}} * gamma + beta\]</div>
<p>The mean and standard-deviation are calculated per-channel over
the mini-batches and gamma and beta are learnable parameter vectors
of size C (where C is the input size).</p>
<p>During training, this layer keeps a running estimate of its computed mean
and variance. The running sum is kept with a default momentum of 0.1.</p>
<p>During evaluation, this running mean/variance is used for normalization.</p>
<p>Because the BatchNorm is done over the <cite>C</cite> dimension, computing statistics
on <cite>(N, H, W)</cite> slices, it’s common terminology to call this Spatial BatchNorm</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>num_features</strong> – num_features from an expected input of
size batch_size x num_features x height x width</p></li>
<li><p><strong>eps</strong> – a value added to the denominator for numerical stability.
Default: 1e-5</p></li>
<li><p><strong>momentum</strong> – the value used for the running_mean and running_var
computation. Default: 0.1</p></li>
<li><p><strong>sync</strong> – a boolean value that when set to <code class="docutils literal notranslate"><span class="pre">True</span></code>, synchronize across
different gpus. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></p></li>
<li><p><strong>activation</strong> – str
Name of the activation functions, one of: <cite>leaky_relu</cite> or <cite>none</cite>.</p></li>
<li><p><strong>slope</strong> – float
Negative slope for the <cite>leaky_relu</cite> activation.</p></li>
</ul>
</dd>
</dl>
<dl>
<dt>Shape:</dt><dd><ul class="simple">
<li><p>Input: <span class="math notranslate nohighlight">\((N, C, H, W)\)</span></p></li>
<li><p>Output: <span class="math notranslate nohighlight">\((N, C, H, W)\)</span> (same shape as input)</p></li>
</ul>
</dd>
<dt>Reference:</dt><dd><dl class="footnote brackets">
<dt class="label" id="id3"><span class="brackets"><a class="fn-backref" href="#id1">1</a></span></dt>
<dd><p>Ioffe, Sergey, and Christian Szegedy. “Batch normalization: Accelerating deep network training by reducing internal covariate shift.” <em>ICML 2015</em></p>
</dd>
<dt class="label" id="id4"><span class="brackets"><a class="fn-backref" href="#id2">2</a></span></dt>
<dd><p>Hang Zhang, Kristin Dana, Jianping Shi, Zhongyue Zhang, Xiaogang Wang, Ambrish Tyagi, and Amit Agrawal. “Context Encoding for Semantic Segmentation.” <em>CVPR 2018</em></p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">SyncBatchNorm</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">net</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">DataParallel</span><span class="p">(</span><span class="n">m</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
<dl class="py method">
<dt id="encoding.nn.SyncBatchNorm.extra_repr">
<code class="sig-name descname">extra_repr</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/encoding/nn/syncbn.html#SyncBatchNorm.extra_repr"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#encoding.nn.SyncBatchNorm.extra_repr" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the extra representation of the module</p>
<p>To print customized extra information, you should reimplement
this method in your own modules. Both single-line and multi-line
strings are acceptable.</p>
</dd></dl>

<dl class="py method">
<dt id="encoding.nn.SyncBatchNorm.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">x</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/encoding/nn/syncbn.html#SyncBatchNorm.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#encoding.nn.SyncBatchNorm.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="batchnorm1d">
<h2><span class="hidden-section">BatchNorm1d</span><a class="headerlink" href="#batchnorm1d" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt id="encoding.nn.BatchNorm1d">
<em class="property">class </em><code class="sig-prename descclassname">encoding.nn.</code><code class="sig-name descname">BatchNorm1d</code><span class="sig-paren">(</span><em class="sig-param"><span class="o">*</span><span class="n">args</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/encoding/nn/syncbn.html#BatchNorm1d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#encoding.nn.BatchNorm1d" title="Permalink to this definition">¶</a></dt>
<dd><div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>BatchNorm1d is deprecated in favor of <a class="reference internal" href="#encoding.nn.SyncBatchNorm" title="encoding.nn.SyncBatchNorm"><code class="xref py py-class docutils literal notranslate"><span class="pre">encoding.nn.SyncBatchNorm</span></code></a>.</p>
</div>
</dd></dl>

</div>
<div class="section" id="batchnorm2d">
<h2><span class="hidden-section">BatchNorm2d</span><a class="headerlink" href="#batchnorm2d" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt id="encoding.nn.BatchNorm2d">
<em class="property">class </em><code class="sig-prename descclassname">encoding.nn.</code><code class="sig-name descname">BatchNorm2d</code><span class="sig-paren">(</span><em class="sig-param"><span class="o">*</span><span class="n">args</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/encoding/nn/syncbn.html#BatchNorm2d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#encoding.nn.BatchNorm2d" title="Permalink to this definition">¶</a></dt>
<dd><div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>BatchNorm2d is deprecated in favor of <a class="reference internal" href="#encoding.nn.SyncBatchNorm" title="encoding.nn.SyncBatchNorm"><code class="xref py py-class docutils literal notranslate"><span class="pre">encoding.nn.SyncBatchNorm</span></code></a>.</p>
</div>
</dd></dl>

</div>
<div class="section" id="batchnorm3d">
<h2><span class="hidden-section">BatchNorm3d</span><a class="headerlink" href="#batchnorm3d" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt id="encoding.nn.BatchNorm3d">
<em class="property">class </em><code class="sig-prename descclassname">encoding.nn.</code><code class="sig-name descname">BatchNorm3d</code><span class="sig-paren">(</span><em class="sig-param"><span class="o">*</span><span class="n">args</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/encoding/nn/syncbn.html#BatchNorm3d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#encoding.nn.BatchNorm3d" title="Permalink to this definition">¶</a></dt>
<dd><div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>BatchNorm3d is deprecated in favor of <a class="reference internal" href="#encoding.nn.SyncBatchNorm" title="encoding.nn.SyncBatchNorm"><code class="xref py py-class docutils literal notranslate"><span class="pre">encoding.nn.SyncBatchNorm</span></code></a>.</p>
</div>
</dd></dl>

</div>
<div class="section" id="inspiration">
<h2><span class="hidden-section">Inspiration</span><a class="headerlink" href="#inspiration" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt id="encoding.nn.Inspiration">
<em class="property">class </em><code class="sig-prename descclassname">encoding.nn.</code><code class="sig-name descname">Inspiration</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">C</span></em>, <em class="sig-param"><span class="n">B</span><span class="o">=</span><span class="default_value">1</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/encoding/nn/encoding.html#Inspiration"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#encoding.nn.Inspiration" title="Permalink to this definition">¶</a></dt>
<dd><p>Inspiration Layer (CoMatch Layer) enables the multi-style transfer in feed-forward
network, which learns to match the target feature statistics during the training.
This module is differentialble and can be inserted in standard feed-forward network
to be learned directly from the loss function without additional supervision.</p>
<div class="math notranslate nohighlight">
\[Y = \phi^{-1}[\phi(\mathcal{F}^T)W\mathcal{G}]\]</div>
<p>Please see the <a class="reference external" href="./experiments/style.html">example of MSG-Net</a>
training multi-style generative network for real-time transfer.</p>
<dl class="simple">
<dt>Reference:</dt><dd><p>Hang Zhang and Kristin Dana. “Multi-style Generative Network for Real-time Transfer.”
<em>arXiv preprint arXiv:1703.06953 (2017)</em></p>
</dd>
</dl>
<dl class="py method">
<dt id="encoding.nn.Inspiration.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">X</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/encoding/nn/encoding.html#Inspiration.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#encoding.nn.Inspiration.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="upsampleconv2d">
<h2><span class="hidden-section">UpsampleConv2d</span><a class="headerlink" href="#upsampleconv2d" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt id="encoding.nn.UpsampleConv2d">
<em class="property">class </em><code class="sig-prename descclassname">encoding.nn.</code><code class="sig-name descname">UpsampleConv2d</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">in_channels</span></em>, <em class="sig-param"><span class="n">out_channels</span></em>, <em class="sig-param"><span class="n">kernel_size</span></em>, <em class="sig-param"><span class="n">stride</span><span class="o">=</span><span class="default_value">1</span></em>, <em class="sig-param"><span class="n">padding</span><span class="o">=</span><span class="default_value">0</span></em>, <em class="sig-param"><span class="n">dilation</span><span class="o">=</span><span class="default_value">1</span></em>, <em class="sig-param"><span class="n">groups</span><span class="o">=</span><span class="default_value">1</span></em>, <em class="sig-param"><span class="n">scale_factor</span><span class="o">=</span><span class="default_value">1</span></em>, <em class="sig-param"><span class="n">bias</span><span class="o">=</span><span class="default_value">True</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/encoding/nn/encoding.html#UpsampleConv2d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#encoding.nn.UpsampleConv2d" title="Permalink to this definition">¶</a></dt>
<dd><p>To avoid the checkerboard artifacts of standard Fractionally-strided Convolution,
we adapt an integer stride convolution but producing a <span class="math notranslate nohighlight">\(2\times 2\)</span> outputs for
each convolutional window.</p>
<a class="reference internal image-reference" href="_images/upconv.png"><img alt="_images/upconv.png" class="align-center" src="_images/upconv.png" style="width: 50%;" /></a>
<dl class="simple">
<dt>Reference:</dt><dd><p>Hang Zhang and Kristin Dana. “Multi-style Generative Network for Real-time Transfer.”
<em>arXiv preprint arXiv:1703.06953 (2017)</em></p>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>in_channels</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – Number of channels in the input image</p></li>
<li><p><strong>out_channels</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – Number of channels produced by the convolution</p></li>
<li><p><strong>kernel_size</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.8)"><em>tuple</em></a>) – Size of the convolving kernel</p></li>
<li><p><strong>stride</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.8)"><em>tuple</em></a><em>, </em><em>optional</em>) – Stride of the convolution. Default: 1</p></li>
<li><p><strong>padding</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.8)"><em>tuple</em></a><em>, </em><em>optional</em>) – Zero-padding added to both sides of the input. Default: 0</p></li>
<li><p><strong>output_padding</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.8)"><em>tuple</em></a><em>, </em><em>optional</em>) – Zero-padding added to one side of the output.
Default: 0</p></li>
<li><p><strong>groups</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a><em>, </em><em>optional</em>) – Number of blocked connections from input channels to output
channels. Default: 1</p></li>
<li><p><strong>bias</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a><em>, </em><em>optional</em>) – If True, adds a learnable bias to the output. Default: True</p></li>
<li><p><strong>dilation</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.8)"><em>tuple</em></a><em>, </em><em>optional</em>) – Spacing between kernel elements. Default: 1</p></li>
<li><p><strong>scale_factor</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – scaling factor for upsampling convolution. Default: 1</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Shape:</dt><dd><ul class="simple">
<li><p>Input: <span class="math notranslate nohighlight">\((N, C_{in}, H_{in}, W_{in})\)</span></p></li>
<li><p>Output: <span class="math notranslate nohighlight">\((N, C_{out}, H_{out}, W_{out})\)</span> where
<span class="math notranslate nohighlight">\(H_{out} = scale * (H_{in} - 1) * stride[0] - 2 * padding[0] + kernel\_size[0] + output\_padding[0]\)</span>
<span class="math notranslate nohighlight">\(W_{out} = scale * (W_{in} - 1) * stride[1] - 2 * padding[1] + kernel\_size[1] + output\_padding[1]\)</span></p></li>
</ul>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Variables</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>~UpsampleConv2d.weight</strong> (<em>Tensor</em>) – the learnable weights of the module of shape
(in_channels, scale * scale * out_channels, kernel_size[0], kernel_size[1])</p></li>
<li><p><strong>~UpsampleConv2d.bias</strong> (<em>Tensor</em>) – the learnable bias of the module of shape (scale * scale * out_channels)</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># With square kernels and equal stride</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">UpsampleCov2d</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">33</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># non-square kernels and unequal stride and with padding</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">UpsampleCov2d</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">33</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># exact output size can be also specified as an argument</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="mi">12</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">downsample</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">upsample</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">UpsampleCov2d</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">h</span> <span class="o">=</span> <span class="n">downsample</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">h</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="go">torch.Size([1, 16, 6, 6])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">upsample</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">output_size</span><span class="o">=</span><span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="go">torch.Size([1, 16, 12, 12])</span>
</pre></div>
</div>
<dl class="py method">
<dt id="encoding.nn.UpsampleConv2d.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">input</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/encoding/nn/encoding.html#UpsampleConv2d.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#encoding.nn.UpsampleConv2d.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="grammatrix">
<h2><span class="hidden-section">GramMatrix</span><a class="headerlink" href="#grammatrix" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt id="encoding.nn.GramMatrix">
<em class="property">class </em><code class="sig-prename descclassname">encoding.nn.</code><code class="sig-name descname">GramMatrix</code><a class="reference internal" href="_modules/encoding/nn/customize.html#GramMatrix"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#encoding.nn.GramMatrix" title="Permalink to this definition">¶</a></dt>
<dd><p>Gram Matrix for a 4D convolutional featuremaps as a mini-batch</p>
<div class="math notranslate nohighlight">
\[\mathcal{G} = \sum_{h=1}^{H_i}\sum_{w=1}^{W_i} \mathcal{F}_{h,w}\mathcal{F}_{h,w}^T\]</div>
<dl class="py method">
<dt id="encoding.nn.GramMatrix.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">y</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/encoding/nn/customize.html#GramMatrix.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#encoding.nn.GramMatrix.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

</dd></dl>

</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="parallel.html" class="btn btn-neutral float-right" title="encoding.parallel" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="experiments/texture.html" class="btn btn-neutral float-left" title="Deep TEN: Deep Texture Encoding Network Example" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2018, Hang Zhang

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>